@article{Falco2015,
Author = {Antonio Falco and Wolfgang Hackbusch and Anthony Nouy},
Title = {Geometric Structures in Tensor Representations},
Year = {2015},
Eprint = {arXiv:1505.03027},
}

@article{lasso,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2346178},
	author = {Robert Tibshirani},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {1},
	pages = {267-288},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Regression Shrinkage and Selection via the Lasso},
	volume = {58},
	year = {1996}
}

@article{Bach2011,
	doi = {10.1561/2200000015},
	url = {https://doi.org/10.1561/2200000015},
	year  = {2011},
	publisher = {Now Publishers},
	volume = {4},
	number = {1},
	pages = {1--106},
	author = {Francis Bach},
	title = {Optimization with Sparsity-Inducing Penalties},
	journal = {Foundations and Trends{\textregistered} in Machine Learning}
}

@article{Chevreuil1,
	author = {Mathilde Chevreuil and RÃ©gis Lebrun and Anthony Nouy and Prashant Rai},
	title = {A Least-Squares Method for Sparse Low Rank Approximation of Multivariate Functions},
	journal = {SIAM/ASA JUQ},
	year = {2015}
}

@book{Hackbusch2012,
	doi = {10.1007/978-3-642-28027-6},
	url = {https://doi.org/10.1007/978-3-642-28027-6},
	year  = {2012},
	publisher = {Springer Berlin Heidelberg},
	author = {Wolfgang Hackbusch},
	title = {Tensor Spaces and Numerical Tensor Calculus}
}

@article{Cohen,
	author    = {Nadav Cohen and Or Sharir and Amnon Shashua},
	title     = {On the Expressive Power of Deep Learning: {A} Tensor Analysis},
	journal   = {CoRR},
	volume    = {abs/1509.05009},
	year      = {2015},
	url       = {http://arxiv.org/abs/1509.05009},
	archivePrefix = {arXiv},
	eprint    = {1509.05009},
	timestamp = {Wed, 07 Jun 2017 14:40:40 +0200},
	biburl    = {http://dblp.org/rec/bib/journals/corr/CohenSS15a},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@misc{Oseledets,
	Author = {Valentin Khrulkov and Alexander Novikov and Ivan Oseledets},
	Title = {Expressive power of recurrent neural networks},
	Year = {2017},
	Eprint = {arXiv:1711.00811},
}

@article{DeLathauwer2000,
	url = {https://doi.org/10.1137/s0895479896305696},
	year  = {2000},
	publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
	author = {Lieven De Lathauwer and Bart De Moor and Joos Vandewalle},
	title = {A Multilinear Singular Value Decomposition},
	journal = {{SIAM} Journal on Matrix Analysis and Applications}
}

@incollection{Nouy2015,
	doi = {10.1007/978-3-319-11259-6_21-1},
	url = {https://doi.org/10.1007/978-3-319-11259-6_21-1},
	year  = {2015},
	publisher = {Springer International Publishing},
	author = {Anthony Nouy},
	title = {Low-Rank Tensor Methods for Model Order Reduction},
	booktitle = {Handbook of Uncertainty Quantification}
}

@article{Bachmayr2016,
	author="Bachmayr, Markus
	and Schneider, Reinhold
	and Uschmajew, Andr{\'e}",
	title="Tensor Networks and Hierarchical Tensors for the Solution of High-Dimensional Partial Differential Equations",
	journal="Foundations of Computational Mathematics",
	year="2016",
}

@article{Grasedyck2010,
	doi = {10.1137/090764189},
	url = {https://doi.org/10.1137/090764189},
	year  = {2010},
	publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
	volume = {31},
	number = {4},
	pages = {2029--2054},
	author = {Lars Grasedyck},
	title = {Hierarchical Singular Value Decomposition of Tensors},
	journal = {{SIAM} Journal on Matrix Analysis and Applications}
}

@incollection{nouy:hal-01262422,
	TITLE = {{Low-rank methods for high-dimensional approximation and model order reduction}},
	AUTHOR = {Nouy, Anthony},
	URL = {https://hal.archives-ouvertes.fr/hal-01262422},
	BOOKTITLE = {{Model Reduction and Approximation: Theory and Algorithms}},
	PUBLISHER = {{SIAM, Philadelphia}},
	YEAR = {2016},
	KEYWORDS = {high-dimensional problems ;  low-rank approximation ;  model order reduction ;  greedy algorithms ;  Proper Generalized Decomposition ;  parameter-dependent equations ;  stochastic equations},
	HAL_ID = {hal-01262422},
	HAL_VERSION = {v1},
}
